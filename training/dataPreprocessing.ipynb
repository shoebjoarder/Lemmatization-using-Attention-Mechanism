{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598731969854",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and preprocessing of Test Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 9.38MB/s]\n2020-08-29 23:28:12 INFO: Downloading default packages for language: en (English)...\n2020-08-29 23:28:12 INFO: File exists: C:\\Users\\shoeb\\stanza_resources\\en\\default.zip.\n2020-08-29 23:28:15 INFO: Finished downloading models and saved to C:\\Users\\shoeb\\stanza_resources.\n2020-08-29 23:28:15 INFO: Loading these models for language: en (English):\n=========================\n| Processor | Package   |\n-------------------------\n| tokenize  | ewt       |\n| pos       | ewt       |\n| lemma     | ewt       |\n| depparse  | ewt       |\n| sentiment | sstplus   |\n| ner       | ontonotes |\n=========================\n\n2020-08-29 23:28:16 INFO: Use device: gpu\n2020-08-29 23:28:16 INFO: Loading: tokenize\n2020-08-29 23:28:18 INFO: Loading: pos\n2020-08-29 23:28:19 INFO: Loading: lemma\n2020-08-29 23:28:19 INFO: Loading: depparse\n2020-08-29 23:28:20 INFO: Loading: sentiment\n2020-08-29 23:28:21 INFO: Loading: ner\n2020-08-29 23:28:21 INFO: Done loading processors!\nLibraries imported!\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import parsingCorpus, writeFile, readFile, text_cleaner\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data imported!\n"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/dataset/dataset_coca.txt\", sep='\\t', usecols=[0,1], header=0, quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "print(\"Data imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Removed empty rows\n"
    }
   ],
   "source": [
    "# Remove empty rows\n",
    "df['Section'].replace('', np.nan, inplace=True)\n",
    "df['section'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['Section', 'section'], axis=0, inplace=True)\n",
    "print(\"Removed empty rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array_split(df, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 358496/358496 [01:54<00:00, 3142.53it/s]\n\n\nDone sentences & lemma!\n"
    }
   ],
   "source": [
    "parsingCorpus(dataset[0], \"sentence\", \"lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset with joined sentences and lemmas\n",
    "sentence = pd.DataFrame(readFile(\"sentence.txt\"), columns=[\"sentence\"])\n",
    "lemma = pd.DataFrame(readFile(\"lemma.txt\"), columns=[\"lemma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Removed empty rows\n"
    }
   ],
   "source": [
    "# Remove empty rows\n",
    "sentence['sentence'].replace('', np.nan, inplace=True)\n",
    "lemma['lemma'].replace('', np.nan, inplace=True)\n",
    "sentence.dropna(subset=['sentence'], axis=0, inplace=True)\n",
    "lemma.dropna(subset=['lemma'], axis=0, inplace=True)\n",
    "print(\"Removed empty rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentence[\"sentence\"].apply(text_cleaner)\n",
    "lemma = lemma[\"lemma\"].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.to_csv(\"./cleaned/sentence.txt\", sep='\\t', index=False, encoding='utf-8')\n",
    "lemma.to_csv(\"./cleaned/lemma.txt\", sep='\\t', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the processed dataset\n",
    "sentence = pd.read_csv(\"./data/cleaned/sentence.txt\", names=[\"InputText\"], quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "lemma = pd.read_csv(\"./data/cleaned/lemma.txt\", names=[\"OutputText\"], quoting=csv.QUOTE_NONE, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.concat([sentence, lemma], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(11991, 2)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sentences shorter than 5 words\n",
    "df = df[df['InputText'].str.split().str.len().gt(5)]\n",
    "df = df[df['InputText'].str.split().str.len().lt(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10220, 2)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/test/test-sample.txt\", sep='\\t', index=False, encoding='utf-8')"
   ]
  }
 ]
}