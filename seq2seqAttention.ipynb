{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597330568776",
   "display_name": "Python 3.8.3 64-bit ('transformers': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "from utils import lemmatize_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/eng-lem.csv\", sep=\"\\t\" , header=0, names=[\"Sentence\", \"Lemmatized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            Sentence  \\\n0  10 Years of Time Team presented a round- up wh...   \n1  13 October 1962 marked the initial working ses...   \n2  1945 Overhauled, Indianapolis joined Vice Admi...   \n3  1965 was also the deadline for AAFSS selection...   \n4  1969 to 1982 The British Telecom\" T symbol log...   \n\n                                          Lemmatized  \n0  10 year of Time team present a round- up what ...  \n1  13 October 1962 mark the initial working sessi...  \n2  1945 overhaul, Indianapolis join vice Admiral ...  \n3  1965 be also the deadline for AAFSS selection,...  \n4  1969 to 1982 the British Telecom\" T symbol log...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>Lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10 Years of Time Team presented a round- up wh...</td>\n      <td>10 year of Time team present a round- up what ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13 October 1962 marked the initial working ses...</td>\n      <td>13 October 1962 mark the initial working sessi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1945 Overhauled, Indianapolis joined Vice Admi...</td>\n      <td>1945 overhaul, Indianapolis join vice Admiral ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1965 was also the deadline for AAFSS selection...</td>\n      <td>1965 be also the deadline for AAFSS selection,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1969 to 1982 The British Telecom\" T symbol log...</td>\n      <td>1969 to 1982 the British Telecom\" T symbol log...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, test_size=0.4)\n",
    "valid, test = train_test_split(valid, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json(\"./data/train.json\", orient=\"records\", lines=True)\n",
    "valid.to_json(\"./data/valid.json\", orient=\"records\", lines=True)\n",
    "test.to_json(\"./data/test.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = Field(init_token=\"<sos>\", eos_token=\"<eos>\", tokenize=tokenize, lower=True)\n",
    "lemmatized = Field(init_token=\"<sos>\", eos_token=\"<eos>\", tokenize=tokenize, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\"Sentence\": (\"sentences\", sentences), \"Lemmatized\": (\"lemmatized\", lemmatized)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = TabularDataset.splits(\n",
    "    path=\"data\", train=\"train.json\", test=\"valid.json\", format=\"json\", fields=fields\n",
    ")\n",
    "\n",
    "test_data = TabularDataset.splits(\n",
    "    path=\"data\", test=\"test.json\", format=\"json\", fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "lemmatized.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, sentences):\n",
    "        src_mask = sentences.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, sentences, lemmatized):\n",
    "        src_seq_length, N = sentences.shape\n",
    "        trg_seq_length, N = lemmatized.shape\n",
    "\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(sentences) + self.src_position_embedding(src_positions))\n",
    "        )\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(lemmatized) + self.trg_position_embedding(trg_positions))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(sentences)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're ready to define everything we need for training our Seq2Seq model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 1000\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(sentences.vocab)\n",
    "trg_vocab_size = len(lemmatized.vocab)\n",
    "embedding_size = 512    # default: 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3  # 6 in paper\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 100\n",
    "forward_expansion = 4\n",
    "src_pad_idx = sentences.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.sentences),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2],\n        [ 103,    5,    5,  838,  790,   47,    0, 2065,   42,    0,   92,    5,\n           74,   16,  688,   54,  263,    0,   22, 4930,   27, 3488,  335, 5588,\n           79,    0,   99,  141,  186,  182,   92,    0],\n        [  40,    0, 1700,    0,  140,  530,   40, 2263, 4543,   23,    5, 3854,\n            5,  719,    6,    7,   10,   63,   45, 4333,   12,   10,   13, 5919,\n           50,    0,  304,  216,   10,  811,  328,   35],\n        [1729, 1480, 7519,    0, 1853, 5289,   51,    5,    6,   56,    0, 1335,\n         1443,    6,    0,   60,    5, 7326,   28, 1536,  155,    5, 1626, 2182,\n           78,   32,   40,    0,  484,   16, 5354,   68],\n        [ 109,  439, 1167,    9,   16,    6, 1329,  108, 2986,  815,  258, 7445,\n            6,   11,    9,    0, 5098,   40,  252,  881,   26,   48,   32,   10,\n           18,    5,   11, 3536,  441,   21,    5, 7300],\n        [5591,  626,   12,  438,    5, 7081,   17,   15,   20,  111,   11,    6,\n         2829, 1785,    0,    6,    6,   51,   16,   78,  921,   65,  840,   28,\n            0,   57,  180,   20,  703,    5, 1593,    0],\n        [  24, 1562, 1844,   10,  675,  158, 2366,   11,    0,   61, 3451, 4082,\n         1540, 4432,   38,   58,   31,  245,    9, 4930, 1529,  361,  700, 2981,\n           19,    6,    7, 3022,    9,  950,    9,    8],\n        [   4,    0,    9,    5,   44,   21, 7874, 7386,    8,   28, 5277, 1044,\n         3649, 1801,  124,   15,   13,    9,    5,   10,    6,    6,   17,    7,\n         1161,    0, 7309,    6, 2337,  745,  210,    5],\n        [  73,   10,  833,  595, 2010,  767, 6897, 3194,    0,    5,   16,    8,\n          245,   64,   49,  281,   11,  298,   50, 3278,    9,    0,    5,    0,\n           10,   35,   10,  153,    6,    6,    7,  851],\n        [   9,   28,    0,   69,    9, 5925,   15,   26,    5,  432,  896,  300,\n           64,    5, 1361,    8,    0,    7, 1742,  150,   16,  154,  374,    6,\n           11,  880,  868,  265,    5,  182,   11,    0],\n        [ 982,  676,   17,  171, 2173,   23,   47,  389,  534, 1891,   36, 1757,\n         1117,  231,   10,   57, 2162, 5160,  833,   48,   79,   34,   10,  121,\n         5016,    0,    6,  579, 2332,   70, 1113,  133],\n        [   6,    5,    5,   14,  234, 3108,    0,    0,    7, 1549,   35,    7,\n           16, 1076, 1657, 4424,  198,    0,  350,   96, 7529, 2441, 1633, 6246,\n           33,   34,    5, 1286,   13, 4462, 6035,    0],\n        [  84, 4784,  502,  231, 5425,   73,   18,   53, 1182,   49,   37,    0,\n          100,  613,   11,   10,    0, 1972,   20,  474, 4568,    0,  407,    0,\n          776,   68, 4194,   16,  995,  323,  131,    0],\n        [7570,    6,    6,    8,   23,  591,  241,   54,   12,   72,    0,    0,\n           28,   16,   24,    5, 2986, 1708, 4115, 1624,   77,    8,   11, 1577,\n           20,   10,  146,    5,   81,  195, 1612,    0],\n        [4338,   36, 1878,  773, 7483,  212,   34, 1965,  205,  189, 1670,   29,\n           85,  257, 2641,  289,   17,   20,  926,    8,  229, 5880,  216,   88,\n          829, 2619,   94, 2350,    0,  215,  285,    7],\n        [2431, 1520, 1778,    6,   10,    9, 2307,   69,   10,   18,  673,  222,\n           15,   45, 1418,   87,  318,    5,   32,   16,    7,  407, 3821,  810,\n          127,    8, 2451,  221,  359,  217,    0, 7068],\n        [5844,    0,  116,   52,  586, 4054,   19,    0,   38, 3300,   34,    9,\n         1357,   34,    8,   12, 4277, 2245,  267,  373,    5,   12, 2762,    9,\n           34,  191,   54, 3954,  111,    8, 7688,    0],\n        [2296,  294, 3417,   25, 4906,    8,   53,   10,   51,   19, 2770,   87,\n           48, 1477,  592,    0,    8, 1174,    6,  657,  215,   69,  158,    0,\n         1083, 3111, 2671,  891,   18,   52,   22,   18],\n        [  10,    8, 1606,  838, 1172, 4617,  425, 1372,    0,    6, 3891,  125,\n         1324,   15,  751,   20,    0,    6,   84,  112,  105, 1115, 3413,  772,\n           15, 5040, 2079,  610,  960,  333,   23, 5834],\n        [1938,  883,   39,   54,    6,   11, 6289,  461,   29,   30,    7,  618,\n          361, 3059, 5264,  711,    4,   17,    0,  232, 1232,  163,    8,    4,\n          374, 4857,  601,    7,    0,   28,   52,   10],\n        [1256, 1733,   11,   24,   17,  494,   21, 2933, 6499, 1124,   10,   15,\n           53,  181,  360, 1741,   73,   11,  419, 1074,   39, 1087, 2127,   24,\n            7, 7375,    8, 7255,    8,  101,   82,  116],\n        [1222,    0, 1291,    0, 1389,  303,   66,    6,   26,    7,  101,   11,\n          481, 1171, 4563,   53, 5181,  752,   25,  306,  340,   17,  639, 3974,\n         3277,   39, 1868,   41,    0,  373, 3290,  236],\n        [ 682,  241,  213, 5452,  677,   77, 1033,   84,    0,   24,  534, 1677,\n          127,   40,  178,   14,   12, 2598,  322,  182, 3762,  367,    7,  200,\n         3262,   37,   42, 2464,   19,    0,  815,   15],\n        [ 107,   32,  311, 1896,    0, 5007,    6,  280,    0, 2788,  129, 1917,\n         2424, 4511,  467,  280,  166, 1800,  569,   45,   25, 1735, 2124,   11,\n         2784,   15,  830, 4303,  131, 1732,  204, 7598],\n        [ 143,   11, 5807, 2622, 1578, 1459, 1603,    0,    0,   12,    0,  431,\n          551,  105,   72, 3496,   56,  178,   72,  143, 5753,   94,  172, 2053,\n           30,    0,   18,    0,   22, 4770,    8,   33],\n        [ 333,  580,   72,   66, 2663, 2667,   77, 7599,  172,  118,   62, 3333,\n         4456,    8,    0,   34, 7693,    8, 2059,   22,   10, 3909,   60, 2182,\n            0, 1928,  524,    8, 1475, 1734,  103,    0],\n        [ 710,  563,  496,  852,   62, 1518, 1245, 6836,   97, 2045, 3299,  764,\n         3559,  532,  278,  752,    0, 6047,  926, 3846,   59, 6675, 2067, 2228,\n          148,    0,   19, 2199, 1299, 1354,   34,   19],\n        [  88,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n            4,    4,    4,    4, 2281,    4,    4,    4,    4,    4,    4,  644,\n            4,    4,    4,    4,    4,    4,    4,    4],\n        [   3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n            3,    3,    3,    3,    3,    3,    3,    3]], device='cuda:0')\n"
    }
   ],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "print(batch.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = sentences.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "sentence = \"A horse is standing under a bridge beside a boat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Epoch 0 / 1000]\n=> Saving checkpoint\nLemmatize example sentence: \n ['young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'young']\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d7c25f14f6df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Forward prop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\shoeb\\Desktop\\Projects\\virtualEnvs\\transformers\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-55bf2c6af176>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sentences, lemmatized)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0msrc_padding_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_src_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "    lemma_sentence = lemmatize_sentence(\n",
    "        model, sentence, sentences, lemmatized, device, max_length=10\n",
    "    )\n",
    "\n",
    "    print(f\"Lemmatize example sentence: \\n {lemma_sentence}\")\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.sentences.to(device)\n",
    "        target = batch.lemmatized.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin.\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}