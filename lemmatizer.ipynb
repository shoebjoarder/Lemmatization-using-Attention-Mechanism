{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597653896327",
   "display_name": "Python 3.8.3 64-bit ('transformers': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing using Stanza Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 6.57MB/s]\n2020-08-17 10:46:06 INFO: Downloading default packages for language: en (English)...\n2020-08-17 10:46:06 INFO: File exists: C:\\Users\\shoeb\\stanza_resources\\en\\default.zip.\n2020-08-17 10:46:09 INFO: Finished downloading models and saved to C:\\Users\\shoeb\\stanza_resources.\n2020-08-17 10:46:09 INFO: Loading these models for language: en (English):\n=========================\n| Processor | Package   |\n-------------------------\n| tokenize  | ewt       |\n| pos       | ewt       |\n| lemma     | ewt       |\n| depparse  | ewt       |\n| sentiment | sstplus   |\n| ner       | ontonotes |\n=========================\n\n2020-08-17 10:46:11 INFO: Use device: gpu\n2020-08-17 10:46:11 INFO: Loading: tokenize\n2020-08-17 10:46:13 INFO: Loading: pos\n2020-08-17 10:46:14 INFO: Loading: lemma\n2020-08-17 10:46:14 INFO: Loading: depparse\n2020-08-17 10:46:15 INFO: Loading: sentiment\n2020-08-17 10:46:15 INFO: Loading: ner\n2020-08-17 10:46:16 INFO: Done loading processors!\n"
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "spacyToken = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset consists on 10,000 sentences. Each line has an id and a sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         id                                           sentence\n9153   9996  The wounded soldiers were taken to a local hos...\n9154   9997  On Thursday, the BART Board is scheduled to co...\n9155   9998           The other problem is the lack of organs.\n9156   9999  Analysts are calling for earnings of 59 cents ...\n9157  10000                        It's a thinking experience.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9153</th>\n      <td>9996</td>\n      <td>The wounded soldiers were taken to a local hos...</td>\n    </tr>\n    <tr>\n      <th>9154</th>\n      <td>9997</td>\n      <td>On Thursday, the BART Board is scheduled to co...</td>\n    </tr>\n    <tr>\n      <th>9155</th>\n      <td>9998</td>\n      <td>The other problem is the lack of organs.</td>\n    </tr>\n    <tr>\n      <th>9156</th>\n      <td>9999</td>\n      <td>Analysts are calling for earnings of 59 cents ...</td>\n    </tr>\n    <tr>\n      <th>9157</th>\n      <td>10000</td>\n      <td>It's a thinking experience.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/english-10k.txt\", sep=\"\\t\", header=None, names=[\"id\", \"sentence\"])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A function to tokenize a sentence using spacy tokenizer. The function takes a sentence as an input argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [token.text for token in spacyToken.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A function to make a sentence shorter. This function takes the sentence and number of tokens a user desires in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortSentence(text, size):\n",
    "    tokens = text[:size]\n",
    "    sentence = \"\"\n",
    "    for txt in tokens:\n",
    "        sentence = sentence + txt + \" \"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The sentences in the dataset is tokenized and then made shorter. The number of tokens in each sentences is chosen 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 9158/9158 [00:01<00:00, 5050.20it/s]\n"
    }
   ],
   "source": [
    "shortText = {'sentence':[]}\n",
    "for i in tqdm(range(len(df))):\n",
    "    text = tokenize(df['sentence'][i])\n",
    "    text = shortSentence(text, 60)\n",
    "    shortText['sentence'].append(text.rstrip())\n",
    "shortText = pd.DataFrame(shortText)\n",
    "shortText.to_csv(r'./data/eng-shortText.txt', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               sentence\n9154  The wounded soldiers were taken to a local hos...\n9155  On Thursday , the BART Board is scheduled to c...\n9156          The other problem is the lack of organs .\n9157  Analysts are calling for earnings of 59 cents ...\n9158                      It 's a thinking experience .",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9154</th>\n      <td>The wounded soldiers were taken to a local hos...</td>\n    </tr>\n    <tr>\n      <th>9155</th>\n      <td>On Thursday , the BART Board is scheduled to c...</td>\n    </tr>\n    <tr>\n      <th>9156</th>\n      <td>The other problem is the lack of organs .</td>\n    </tr>\n    <tr>\n      <th>9157</th>\n      <td>Analysts are calling for earnings of 59 cents ...</td>\n    </tr>\n    <tr>\n      <th>9158</th>\n      <td>It 's a thinking experience .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/eng-shortText.txt\", sep=\"\\t\", header=None, names=[\"sentence\"])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Maximum number of tokens in a sentence: 61\n"
    }
   ],
   "source": [
    "token_lens = []\n",
    "a = 0\n",
    "for txt in df.sentence:\n",
    "  tokens = tokenize(txt)\n",
    "  b = len(tokens)\n",
    "  if b > a:\n",
    "    a = b\n",
    "  token_lens.append(len(tokens))\n",
    "print(\"Maximum number of tokens in a sentence:\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a function that will process the lemmatization using stanfordnlp stanza library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLemmatization(text):\n",
    "    word = \"\"\n",
    "    lemma = \"\"\n",
    "    punc= set(string.punctuation)\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sentences:\n",
    "        for wrd in sent.words:\n",
    "            c = str(wrd.text)\n",
    "            d = str(wrd.lemma)\n",
    "            if any(char in punc for char in d)==True:\n",
    "                word = word.lstrip()\n",
    "                lemma = lemma.lstrip()\n",
    "                word += c + \" \"\n",
    "                lemma += d + \" \"\n",
    "            else:\n",
    "                word += c + \" \"\n",
    "                lemma += d + \" \"\n",
    "    return word, lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The new dataset with 60 tokens in each sentence will be used as input. Each sentence will be passed and create the lemmatized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 9159/9159 [12:13<00:00, 12.49it/s]\n"
    }
   ],
   "source": [
    "parsed_text = {'sentence':[], 'lemma':[]}\n",
    "for i in tqdm(range(len(df))):\n",
    "    word, lemma = processLemmatization(df['sentence'][i])\n",
    "    #extract text and lemma\n",
    "    parsed_text['sentence'].append(word.rstrip())\n",
    "    parsed_text['lemma'].append(lemma.rstrip())\n",
    "text = pd.DataFrame(parsed_text)\n",
    "# text.to_csv(r'./data/eng-lem.txt', sep='\\t', index = False)\n",
    "text.to_csv(r'./data/eng-lem.txt', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               sentence  \\\n9155  The wounded soldiers were taken to a local hos...   \n9156  On Thursday , the BART Board is scheduled to c...   \n9157          The other problem is the lack of organs .   \n9158  Analysts are calling for earnings of 59 cents ...   \n9159                      It 's a thinking experience .   \n\n                                                  lemma  \n9155  the wounded soldier be take to a local hospita...  \n9156  on Thursday , the BART Board be schedule to co...  \n9157           the other problem be the lack of organ .  \n9158  analyst be call for earning of 59 cent a share...  \n9159                      it be a thinking experience .  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9155</th>\n      <td>The wounded soldiers were taken to a local hos...</td>\n      <td>the wounded soldier be take to a local hospita...</td>\n    </tr>\n    <tr>\n      <th>9156</th>\n      <td>On Thursday , the BART Board is scheduled to c...</td>\n      <td>on Thursday , the BART Board be schedule to co...</td>\n    </tr>\n    <tr>\n      <th>9157</th>\n      <td>The other problem is the lack of organs .</td>\n      <td>the other problem be the lack of organ .</td>\n    </tr>\n    <tr>\n      <th>9158</th>\n      <td>Analysts are calling for earnings of 59 cents ...</td>\n      <td>analyst be call for earning of 59 cent a share...</td>\n    </tr>\n    <tr>\n      <th>9159</th>\n      <td>It 's a thinking experience .</td>\n      <td>it be a thinking experience .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/eng-lem.txt\", sep=\"\\t\", header=None, names=[\"sentence\", \"lemma\"])\n",
    "df.tail()"
   ]
  }
 ]
}