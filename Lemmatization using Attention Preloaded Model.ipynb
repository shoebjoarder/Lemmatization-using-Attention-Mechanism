{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598868240868",
   "display_name": "Python 3.8.3 64-bit ('transformers': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization using Attention Mechanism Preloaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 20.4MB/s]\n2020-08-31 13:13:17 INFO: Downloading default packages for language: en (English)...\n2020-08-31 13:13:18 INFO: File exists: C:\\Users\\shoeb\\stanza_resources\\en\\default.zip.\n2020-08-31 13:13:21 INFO: Finished downloading models and saved to C:\\Users\\shoeb\\stanza_resources.\n2020-08-31 13:13:21 INFO: Loading these models for language: en (English):\n=========================\n| Processor | Package   |\n-------------------------\n| tokenize  | ewt       |\n| pos       | ewt       |\n| lemma     | ewt       |\n| depparse  | ewt       |\n| sentiment | sstplus   |\n| ner       | ontonotes |\n=========================\n\n2020-08-31 13:13:22 INFO: Use device: gpu\n2020-08-31 13:13:22 INFO: Loading: tokenize\n2020-08-31 13:13:24 INFO: Loading: pos\n2020-08-31 13:13:25 INFO: Loading: lemma\n2020-08-31 13:13:25 INFO: Loading: depparse\n2020-08-31 13:13:25 INFO: Loading: sentiment\n2020-08-31 13:13:26 INFO: Loading: ner\n2020-08-31 13:13:27 INFO: Done loading processors!\n"
    }
   ],
   "source": [
    "# Import library\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import pickle\n",
    "import csv\n",
    "from utils import process_sentence, load_obj, save_obj, save_checkpoint, load_checkpoint, predict, evaluate\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_input = spacy.load(\"en\")\n",
    "spacy_output = spacy.load(\"en\")\n",
    "\n",
    "def tokenize_input(text):\n",
    "    return [token.text for token in spacy_input.tokenizer(text)]\n",
    "\n",
    "def tokenize_output(text):\n",
    "    return [token.text for token in spacy_output.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = Field(init_token=\"<sos>\", eos_token=\"<eos>\", tokenize=tokenize_input, lower=True)\n",
    "outputText = Field(init_token=\"<sos>\", eos_token=\"<eos>\", tokenize=tokenize_output, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\"InputText\": (\"inputText\", inputText), \"OutputText\": (\"outputText\", outputText)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText.vocab = load_obj(\"inputText\")\n",
    "outputText.vocab = load_obj(\"outputText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, inputText):\n",
    "        src_mask = inputText.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, inputText, outputText):\n",
    "        src_seq_length, N = inputText.shape\n",
    "        trg_seq_length, N = outputText.shape\n",
    "\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(inputText) + self.src_position_embedding(src_positions))\n",
    "        )\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(outputText) + self.trg_position_embedding(trg_positions))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(inputText)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're ready to define everything we need for training our Seq2Seq model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "load_model = True\n",
    "save_model = True\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(inputText.vocab)\n",
    "trg_vocab_size = len(outputText.vocab)\n",
    "embedding_size = 512    # default: 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3  # 6 in paper\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 70\n",
    "forward_expansion = 4\n",
    "src_pad_idx = inputText.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = inputText.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=> Loading checkpoint\n"
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"./model/my_checkpoint.pth.tar\"), model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['the', 'feasibility', 'study', 'estimate', 'that', 'it', 'would', 'take', 'passenger', 'about', 'four', 'minute', 'to', 'cross', 'the', 'potomac', 'river', 'on', 'the', 'gondola', '.']\n"
    }
   ],
   "source": [
    "src = \"The feasibility study estimates that it would take passengers about four minutes to cross the Potomac River on the gondola.\"\n",
    "\n",
    "prediction = process_sentence(model, src, inputText, outputText, device)\n",
    "prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10220, 2)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Loading the test dataset\n",
    "test_data = pd.read_csv(\"./training/test/test-sample.txt\", header=0, names=['InputText', 'OutputText'], sep='\\t', encoding='utf-8')\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              InputText  \\\n0     The stenciled images are of things an insect r...   \n1     The justice of any particular holding is a mat...   \n2          The major psychiatric illnesses are diseases   \n3     Anyone who went through this curriculum got a ...   \n4     It was in that car that Max and I would drive ...   \n...                                                 ...   \n1995  And it would consider how to get students to a...   \n1996  After the third condition another trials of th...   \n1997  Santayana was criticized by pragmatists for hi...   \n1998  He would not assume an adversarial relationshi...   \n1999  A recent theoretical paper Richards suggested ...   \n\n                                             OutputText  \n0     the stenciled image be of thing a insect repea...  \n1     the justice of any particular holding be a mat...  \n2              the major psychiatric illness be disease  \n3     anyone who go through this curriculum get a li...  \n4     it be in that car that maximum and i would dri...  \n...                                                 ...  \n1995  and it would consider how to get student to ac...  \n1996  after the third condition another trial of the...  \n1997  santayana be criticize by pragmatist for his i...  \n1998  he would not assume a adversarial relationship...  \n1999  a recent theoretical paper richards suggest th...  \n\n[2000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InputText</th>\n      <th>OutputText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The stenciled images are of things an insect r...</td>\n      <td>the stenciled image be of thing a insect repea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The justice of any particular holding is a mat...</td>\n      <td>the justice of any particular holding be a mat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The major psychiatric illnesses are diseases</td>\n      <td>the major psychiatric illness be disease</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Anyone who went through this curriculum got a ...</td>\n      <td>anyone who go through this curriculum get a li...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>It was in that car that Max and I would drive ...</td>\n      <td>it be in that car that maximum and i would dri...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>And it would consider how to get students to a...</td>\n      <td>and it would consider how to get student to ac...</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>After the third condition another trials of th...</td>\n      <td>after the third condition another trial of the...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>Santayana was criticized by pragmatists for hi...</td>\n      <td>santayana be criticize by pragmatist for his i...</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>He would not assume an adversarial relationshi...</td>\n      <td>he would not assume a adversarial relationship...</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>A recent theoretical paper Richards suggested ...</td>\n      <td>a recent theoretical paper richards suggest th...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "test_data = test_data.sample(n=2000)\n",
    "# test_data = test_data[:1000]\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.drop(['index'], axis=1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2000/2000 [24:42<00:00,  1.35it/s]\n"
    }
   ],
   "source": [
    "# Making predictions\n",
    "predictions, targets = predict(test_data[\"InputText\"], test_data[\"OutputText\"], model, inputText, outputText, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy 22.20\n"
    }
   ],
   "source": [
    "# Evaluating with score\n",
    "score = evaluate(targets, predictions)\n",
    "print(f\"Accuracy {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 13.6MB/s]\n2020-08-31 13:38:17 INFO: Downloading default packages for language: en (English)...\n2020-08-31 13:38:17 INFO: File exists: C:\\Users\\shoeb\\stanza_resources\\en\\default.zip.\n2020-08-31 13:38:20 INFO: Finished downloading models and saved to C:\\Users\\shoeb\\stanza_resources.\n2020-08-31 13:38:20 WARNING: Can not find mwt: default from official model list. Ignoring it.\n2020-08-31 13:38:20 INFO: Loading these models for language: en (English):\n=======================\n| Processor | Package |\n-----------------------\n| tokenize  | ewt     |\n| pos       | ewt     |\n| lemma     | ewt     |\n=======================\n\n2020-08-31 13:38:20 INFO: Use device: gpu\n2020-08-31 13:38:20 INFO: Loading: tokenize\n2020-08-31 13:38:20 INFO: Loading: pos\n2020-08-31 13:38:21 INFO: Loading: lemma\n2020-08-31 13:38:21 INFO: Done loading processors!\n"
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "\n",
    "nlp = stanza.Pipeline(processors = \"tokenize,mwt,lemma,pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lemma(df):\n",
    "    prediction = []\n",
    "    for iter in tqdm(range(len(df))):\n",
    "        doc = nlp(df[iter])\n",
    "        for sent in doc.sentences:\n",
    "            lemma = []\n",
    "            for wrd in sent.words:\n",
    "                lemma.append(str(wrd.lemma).lower())\n",
    "                # word.append(str(wrd.text))\n",
    "            prediction.append(lemma)\n",
    "            # target.append(word)\n",
    "        #return a dataframe\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionStanza(data):\n",
    "    prediction = []\n",
    "    for iter in tqdm(range(len(data))):\n",
    "        doc = nlp(data[iter])\n",
    "        text = \"\"\n",
    "        for sent in doc.sentences:\n",
    "            for wrd in sent.words:\n",
    "                text = text + wrd.lemma + \" \"\n",
    "        lemma = text.split()\n",
    "        prediction.append(lemma)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2000/2000 [01:12<00:00, 27.54it/s]\n"
    }
   ],
   "source": [
    "predictStanza = predictionStanza(test_data[\"InputText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy 23.60\n"
    }
   ],
   "source": [
    "score = evaluate(targets, predictStanza)\n",
    "print(f\"Accuracy {score * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Started Reading JSON file which contains multiple JSON document\n"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "testData = []\n",
    "print(\"Started Reading JSON file which contains multiple JSON document\")\n",
    "with open('./data/test.json') as f:\n",
    "    for jsonObj in f:\n",
    "        testDict = json.loads(jsonObj)\n",
    "        testData.append(testDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 916/916 [00:00<00:00, 918694.99it/s]\n"
    }
   ],
   "source": [
    "inText = []\n",
    "outText = []\n",
    "for iter in tqdm(range(len(testData))):\n",
    "    inText.append([testData[iter]['InputText']])\n",
    "    outText.append([testData[iter]['OutputText']])\n",
    "# testData[0][\"OutputText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['oil start flow from the Swanson River oil field in 1957 .']"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "outText[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Oil started flowing from the Swanson River oil field in 1957 .']"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "inText[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = pd.DataFrame(inText)\n",
    "targetData = pd.DataFrame(outText)\n",
    "inputData.columns = [\"InputText\"]\n",
    "targetData.columns = [\"OutputText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           InputText\n0  Oil started flowing from the Swanson River oil...\n1  According to the National Institute of Mental ...\n2  If you did n't find what you were looking for ...\n3  We were terrified when the epidemic started be...\n4  It 's this detection technique that caught Tyl...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InputText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Oil started flowing from the Swanson River oil...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>According to the National Institute of Mental ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If you did n't find what you were looking for ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>We were terrified when the epidemic started be...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>It 's this detection technique that caught Tyl...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "inputData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Oil started flowing from the Swanson River oil field in 1957 .'"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "inputData[\"InputText\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = targetData[\"OutputText\"].apply(tokenize_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['oil',\n 'start',\n 'flow',\n 'from',\n 'the',\n 'Swanson',\n 'River',\n 'oil',\n 'field',\n 'in',\n '1957',\n '.']"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "target[0]"
   ]
  }
 ]
}